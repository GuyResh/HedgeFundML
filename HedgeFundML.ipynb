{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0bf2f320-541f-4990-bd0c-0824fabee9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Hedge Fund ML (sandbox/playground/harness)\n",
    "#\n",
    "# (c)2022 Guy Resh\n",
    "#\n",
    "# - start fund on 1/1/2020 with $1B seed capital\n",
    "# - +$1B additional seed injections on 1/1/2021 & 1/1/2022\n",
    "# - tradable securities selected from Dow 30, Nasdaq 100 & S&P 500 (528 unique; less 8 partial)\n",
    "# - maintain portfolio with 50-100 \"best\" stocks\n",
    "# - maintain diversification with 5-10 different industry sectors\n",
    "# - generate Buy-and-Hold P/L % statistics for EOY 2020, 2021 & YTD 2022 for all 520 securities (long-only)\n",
    "# - generate \"opportunity\" (measured move statistics) P/L using fractal-based reversal pivot points for all 520 securities (long-only)\n",
    "# - maintain 1%-5% minimum monthly profit (stop at second consecutive losing month or if drawdown exceeds 10%)\n",
    "# - features/strategies based on CCI, DC, KR, LRBO, RSI, VWAP, Half/SuperTrend, Volume, Velocity/Momentum, etc.\n",
    "# - 0% commissions assumed (though can/should be accounted for at some point)\n",
    "# - whole share purchases-only (no fractional; round quantities down to nearest 100?)\n",
    "# - generate portfolio scenarios that rebalance daily, weekly, monthly and quarterly\n",
    "# - split-handling?\n",
    "# - dividend income inclusion?\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1611fb31-abef-4dfd-8ff5-bff1df18f44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "from finvizfinance.quote import finvizfinance\n",
    "import pyiqfeed as iq\n",
    "from pyiqfeed.field_readers import read_posix_ts, date_us_to_datetime, datetime_to_yyyymmdd_hhmmss, us_since_midnight_to_time\n",
    "\n",
    "from math import floor\n",
    "from tqdm.notebook import tqdm\n",
    "from termcolor import colored as cl\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.rcParams['figure.figsize'] = (20,10)\n",
    "\n",
    "pd.set_option( 'display.max_rows', None )\n",
    "pd.set_option( 'display.max_columns', None )\n",
    "pd.set_option( 'display.width', None )\n",
    "pd.set_option( 'display.max_colwidth', None )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ec8e8425-ed70-49d1-b065-24ffb57bdbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if sys.platform == 'linux':\n",
    "  home = '/mnt/f/db/IQFeed/'\n",
    "else:\n",
    "  home = 'F:/db/IQFeed/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "685f8fe2-d00a-4e51-8984-ce234ac9bb8e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "520 unique symbols\n"
     ]
    }
   ],
   "source": [
    "Dow30Syms = [\n",
    "\"AAPL\", \"AMGN\", \"AXP\", \"BA\", \"CAT\", \"CRM\", \"CSCO\", \"CVX\", \"DIS\", \"DOW\",\n",
    "\"GS\", \"HD\", \"HON\", \"IBM\", \"INTC\", \"JNJ\", \"JPM\", \"KO\", \"MCD\", \"MMM\",\n",
    "\"MRK\", \"MSFT\", \"NKE\", \"PG\", \"TRV\", \"UNH\", \"V\", \"VZ\", \"WBA\", \"WMT\"\n",
    "]\n",
    "\n",
    "Nasdaq100Syms = [ # 102\n",
    "\"AAPL\", \"ABNB\", \"ADBE\", \"ADI\", \"ADP\", \"ADSK\", \"AEP\", \"ALGN\", \"AMAT\", \"AMD\",\n",
    "\"AMGN\", \"AMZN\", \"ANSS\", \"ASML\", \"ATVI\", \"AVGO\", \"AZN\", \"BIDU\", \"BIIB\", \"BKNG\",\n",
    "\"CDNS\", \"CEG\", \"CHTR\", \"CMCSA\", \"COST\", \"CPRT\", \"CRWD\", \"CSCO\", \"CSX\", \"CTAS\",\n",
    "\"CTSH\", \"DDOG\", \"DLTR\", \"DOCU\", \"DXCM\", \"EA\", \"EBAY\", \"EXC\", \"FAST\", \"FB\",\n",
    "\"FISV\", \"FTNT\", \"GILD\", \"GOOG\", \"GOOGL\", \"HON\", \"IDXX\", \"ILMN\", \"INTC\", \"INTU\",\n",
    "\"ISRG\", \"JD\", \"KDP\", \"KHC\", \"KLAC\", \"LCID\", \"LRCX\", \"LULU\", \"MAR\", \"MCHP\",\n",
    "\"MDLZ\", \"MELI\", \"MNST\", \"MRNA\", \"MRVL\", \"MSFT\", \"MTCH\", \"MU\", \"NFLX\", \"NTES\",\n",
    "\"NVDA\", \"NXPI\", \"ODFL\", \"OKTA\", \"ORLY\", \"PANW\", \"PAYX\", \"PCAR\", \"PDD\", \"PEP\",\n",
    "\"PYPL\", \"QCOM\", \"REGN\", \"ROST\", \"SBUX\", \"SGEN\", \"SIRI\", \"SNPS\", \"SPLK\", \"SWKS\",\n",
    "\"TEAM\", \"TMUS\", \"TSLA\", \"TXN\", \"VRSK\", \"VRSN\", \"VRTX\", \"WBA\", \"WDAY\", \"XEL\",\n",
    "\"ZM\", \"ZS\"\n",
    "]\n",
    "\n",
    "SP500Syms = [ # 504\n",
    "\"A\", \"AAL\", \"AAP\", \"AAPL\", \"ABBV\", \"ABC\", \"ABMD\", \"ABT\", \"ACN\", \"ADBE\",\n",
    "\"ADI\", \"ADM\", \"ADP\", \"ADSK\", \"AEE\", \"AEP\", \"AES\", \"AFL\", \"AIG\", \"AIZ\",\n",
    "\"AJG\", \"AKAM\", \"ALB\", \"ALGN\", \"ALK\", \"ALL\", \"ALLE\", \"AMAT\", \"AMCR\", \"AMD\",\n",
    "\"AME\", \"AMGN\", \"AMP\", \"AMT\", \"AMZN\", \"ANET\", \"ANSS\", \"ANTM\", \"AON\", \"AOS\",\n",
    "\"APA\", \"APD\", \"APH\", \"APTV\", \"ARE\", \"ATO\", \"ATVI\", \"AVB\", \"AVGO\", \"AVY\",\n",
    "\"AWK\", \"AXP\", \"AZO\", \"BA\", \"BAC\", \"BAX\", \"BBWI\", \"BBY\", \"BDX\", \"BEN\",\n",
    "\"BF.B\", \"BIIB\", \"BIO\", \"BK\", \"BKNG\", \"BKR\", \"BLK\", \"BLL\", \"BMY\", \"BR\",\n",
    "\"BRK.B\", \"BRO\", \"BSX\", \"BWA\", \"BXP\", \"C\", \"CAG\", \"CAH\", \"CARR\", \"CAT\",\n",
    "\"CB\", \"CBOE\", \"CBRE\", \"CCI\", \"CCL\", \"CDAY\", \"CDNS\", \"CDW\", \"CE\", \"CEG\",\n",
    "\"CERN\", \"CF\", \"CFG\", \"CHD\", \"CHRW\", \"CHTR\", \"CI\", \"CINF\", \"CL\", \"CLX\",\n",
    "\"CMA\", \"CMCSA\", \"CME\", \"CMG\", \"CMI\", \"CMS\", \"CNC\", \"CNP\", \"COF\", \"COO\",\n",
    "\"COP\", \"COST\", \"CPB\", \"CPRT\", \"CPT\", \"CRL\", \"CRM\", \"CSCO\", \"CSX\", \"CTAS\",\n",
    "\"CTLT\", \"CTRA\", \"CTSH\", \"CTVA\", \"CTXS\", \"CVS\", \"CVX\", \"CZR\", \"D\", \"DAL\",\n",
    "\"DD\", \"DE\", \"DFS\", \"DG\", \"DGX\", \"DHI\", \"DHR\", \"DIS\", \"DISH\", \"DLR\",\n",
    "\"DLTR\", \"DOV\", \"DOW\", \"DPZ\", \"DRE\", \"DRI\", \"DTE\", \"DUK\", \"DVA\", \"DVN\",\n",
    "\"DXC\", \"DXCM\", \"EA\", \"EBAY\", \"ECL\", \"ED\", \"EFX\", \"EIX\", \"EL\", \"EMN\",\n",
    "\"EMR\", \"ENPH\", \"EOG\", \"EPAM\", \"EQIX\", \"EQR\", \"ES\", \"ESS\", \"ETN\", \"ETR\",\n",
    "\"ETSY\", \"EVRG\", \"EW\", \"EXC\", \"EXPD\", \"EXPE\", \"EXR\", \"F\", \"FANG\", \"FAST\",\n",
    "\"FB\", \"FBHS\", \"FCX\", \"FDS\", \"FDX\", \"FE\", \"FFIV\", \"FIS\", \"FISV\", \"FITB\",\n",
    "\"FLT\", \"FMC\", \"FOX\", \"FOXA\", \"FRC\", \"FRT\", \"FTNT\", \"FTV\", \"GD\", \"GE\",\n",
    "\"GILD\", \"GIS\", \"GL\", \"GLW\", \"GM\", \"GNRC\", \"GOOG\", \"GOOGL\", \"GPC\", \"GPN\",\n",
    "\"GRMN\", \"GS\", \"GWW\", \"HAL\", \"HAS\", \"HBAN\", \"HCA\", \"HD\", \"HES\", \"HIG\",\n",
    "\"HII\", \"HLT\", \"HOLX\", \"HON\", \"HPE\", \"HPQ\", \"HRL\", \"HSIC\", \"HST\", \"HSY\",\n",
    "\"HUM\", \"HWM\", \"IBM\", \"ICE\", \"IDXX\", \"IEX\", \"IFF\", \"ILMN\", \"INCY\", \"INTC\",\n",
    "\"INTU\", \"IP\", \"IPG\", \"IPGP\", \"IQV\", \"IR\", \"IRM\", \"ISRG\", \"IT\", \"ITW\",\n",
    "\"IVZ\", \"J\", \"JBHT\", \"JCI\", \"JKHY\", \"JNJ\", \"JNPR\", \"JPM\", \"K\", \"KEY\",\n",
    "\"KEYS\", \"KHC\", \"KIM\", \"KLAC\", \"KMB\", \"KMI\", \"KMX\", \"KO\", \"KR\", \"L\",\n",
    "\"LDOS\", \"LEN\", \"LH\", \"LHX\", \"LIN\", \"LKQ\", \"LLY\", \"LMT\", \"LNC\", \"LNT\",\n",
    "\"LOW\", \"LRCX\", \"LUMN\", \"LUV\", \"LVS\", \"LW\", \"LYB\", \"LYV\", \"MA\", \"MAA\",\n",
    "\"MAR\", \"MAS\", \"MCD\", \"MCHP\", \"MCK\", \"MCO\", \"MDLZ\", \"MDT\", \"MET\", \"MGM\",\n",
    "\"MHK\", \"MKC\", \"MKTX\", \"MLM\", \"MMC\", \"MMM\", \"MNST\", \"MO\", \"MOH\", \"MOS\",\n",
    "\"MPC\", \"MPWR\", \"MRK\", \"MRNA\", \"MRO\", \"MS\", \"MSCI\", \"MSFT\", \"MSI\", \"MTB\",\n",
    "\"MTCH\", \"MTD\", \"MU\", \"NCLH\", \"NDAQ\", \"NDSN\", \"NEE\", \"NEM\", \"NFLX\", \"NI\",\n",
    "\"NKE\", \"NLOK\", \"NLSN\", \"NOC\", \"NOW\", \"NRG\", \"NSC\", \"NTAP\", \"NTRS\", \"NUE\",\n",
    "\"NVDA\", \"NVR\", \"NWL\", \"NWS\", \"NWSA\", \"NXPI\", \"O\", \"ODFL\", \"OGN\", \"OKE\",\n",
    "\"OMC\", \"ORCL\", \"ORLY\", \"OTIS\", \"OXY\", \"PARA\", \"PAYC\", \"PAYX\", \"PCAR\", \"PEAK\",\n",
    "\"PEG\", \"PENN\", \"PEP\", \"PFE\", \"PFG\", \"PG\", \"PGR\", \"PH\", \"PHM\", \"PKG\",\n",
    "\"PKI\", \"PLD\", \"PM\", \"PNC\", \"PNR\", \"PNW\", \"POOL\", \"PPG\", \"PPL\", \"PRU\",\n",
    "\"PSA\", \"PSX\", \"PTC\", \"PVH\", \"PWR\", \"PXD\", \"PYPL\", \"QCOM\", \"QRVO\", \"RCL\",\n",
    "\"RE\", \"REG\", \"REGN\", \"RF\", \"RHI\", \"RJF\", \"RL\", \"RMD\", \"ROK\", \"ROL\",\n",
    "\"ROP\", \"ROST\", \"RSG\", \"RTX\", \"SBAC\", \"SBNY\", \"SBUX\", \"SCHW\", \"SEDG\", \"SEE\",\n",
    "\"SHW\", \"SIVB\", \"SJM\", \"SLB\", \"SNA\", \"SNPS\", \"SO\", \"SPG\", \"SPGI\", \"SRE\",\n",
    "\"STE\", \"STT\", \"STX\", \"STZ\", \"SWK\", \"SWKS\", \"SYF\", \"SYK\", \"SYY\", \"T\",\n",
    "\"TAP\", \"TDG\", \"TDY\", \"TECH\", \"TEL\", \"TER\", \"TFC\", \"TFX\", \"TGT\", \"TJX\",\n",
    "\"TMO\", \"TMUS\", \"TPR\", \"TRMB\", \"TROW\", \"TRV\", \"TSCO\", \"TSLA\", \"TSN\", \"TT\",\n",
    "\"TTWO\", \"TWTR\", \"TXN\", \"TXT\", \"TYL\", \"UA\", \"UAA\", \"UAL\", \"UDR\", \"UHS\",\n",
    "\"ULTA\", \"UNH\", \"UNP\", \"UPS\", \"URI\", \"USB\", \"V\", \"VFC\", \"VLO\", \"VMC\",\n",
    "\"VNO\", \"VRSK\", \"VRSN\", \"VRTX\", \"VTR\", \"VTRS\", \"VZ\", \"WAB\", \"WAT\", \"WBA\",\n",
    "\"WBD\", \"WDC\", \"WEC\", \"WELL\", \"WFC\", \"WHR\", \"WM\", \"WMB\", \"WMT\", \"WRB\",\n",
    "\"WRK\", \"WST\", \"WTW\", \"WY\", \"WYNN\", \"XEL\", \"XOM\", \"XRAY\", \"XYL\", \"YUM\",\n",
    "\"ZBH\", \"ZBRA\", \"ZION\", \"ZTS\"\n",
    "]\n",
    "\n",
    "uniqueSymbols = np.sort( np.unique( np.array( Dow30Syms + Nasdaq100Syms + SP500Syms ))).tolist()\n",
    "\n",
    "#\n",
    "# Don't include these symbols that don't have a full complement of data (IPO'd after 1/1/2020?)\n",
    "#\n",
    "#ABNB.pkl: 2020-12-10 13:40:00\n",
    "#CARR.pkl: 2020-03-19 15:45:00\n",
    "#CEG.pkl: 2022-01-19 10:25:00\n",
    "#LCID.pkl: 2020-09-18 09:40:00\n",
    "#OGN.pkl: 2021-05-14 11:35:00\n",
    "#OTIS.pkl: 2020-03-19 11:40:00\n",
    "#VTRS.pkl: 2020-11-12 09:35:00\n",
    "#WBD.pkl: 2022-04-04 09:35:00\n",
    "partialSymbols = [\"ABNB\",\"CARR\",\"CEG\",\"LCID\",\"OGN\",\"OTIS\",\"VTRS\",\"WBD\"]\n",
    "for symbol in partialSymbols:\n",
    "  uniqueSymbols.remove( symbol ) \n",
    "\n",
    "print( len( uniqueSymbols ), 'unique symbols' ) # 528-8\n",
    "#print( list( uniqueSymbols ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ef1fdd10-64c5-441f-a666-4e4177af4e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 520 ] fundamental data loaded...\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Download/persist (some) fundamental data from finviz\n",
    "#\n",
    "fundamentals = {}\n",
    "fn_json = 'data/fundamentals.json'\n",
    "if not os.path.isfile( fn_json ):\n",
    "  for symbol in uniqueSymbols:\n",
    "    finvizSymbol = symbol.replace( '.', '-' )\n",
    "    try:\n",
    "      stock = finvizfinance( finvizSymbol )\n",
    "    except (RuntimeError, TypeError, NameError):\n",
    "      pass\n",
    "    finvizFundamentals = stock.ticker_fundament()\n",
    "    fundamentals[symbol] = {\n",
    "      'Company':   finvizFundamentals['Company'],\n",
    "      'Sector':    finvizFundamentals['Sector'],\n",
    "      'Industry':  finvizFundamentals['Industry'],\n",
    "      'MarketCap': finvizFundamentals['Market Cap']\n",
    "    }\n",
    "    #print( symbol, fundamentals[symbol] )\n",
    "\n",
    "  dfFundamentals = pd.DataFrame.from_dict( fundamentals, orient=\"index\" )\n",
    "  #print( dfFundamentals.info() )\n",
    "  #print( dfFundamentals )\n",
    "\n",
    "  jsonObj = json.loads( dfFundamentals.to_json( orient=\"index\" ))\n",
    "  jsonFundamentals = json.dumps( jsonObj, indent=2 )\n",
    "  print( jsonFundamentals )\n",
    "  with open( fn_json, \"w\" ) as f:\n",
    "    f.write( jsonFundamentals )\n",
    "\n",
    "dfFundamentals = pd.read_json( fn_json, orient=\"index\" )\n",
    "#print( dfFundamentals[0:10] )\n",
    "print( \"[\", len( dfFundamentals ), \"] fundamental data loaded...\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "779f675f-059d-4fe5-8f35-530823a03e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Download 5 minute bars from IQFeed from bgn_prd to end_prd and return as a Pandas DataFrame\n",
    "#\n",
    "def get_historical_data( symbol, bgn_prd: datetime.datetime, end_prd: datetime.datetime ):\n",
    "\n",
    "  print( 'get_historical_data(', symbol, ',', datetime_to_yyyymmdd_hhmmss( bgn_prd ), ',', datetime_to_yyyymmdd_hhmmss( end_prd ), ')' )\n",
    "\n",
    "  histConn = iq.HistoryConn( name=\"pyiqfeed\" )\n",
    "  histConn.connect()\n",
    "  histListener = iq.VerboseIQFeedListener( 'History Tick Listener' )\n",
    "  histConn.add_listener( histListener )\n",
    "\n",
    "  #dt, tm = read_posix_ts( bp_str )\n",
    "  #bgn_prd = date_us_to_datetime( dt, tm )\n",
    "  #dt, tm = read_posix_ts( ep_str )\n",
    "  #end_prd = date_us_to_datetime( dt, tm )\n",
    "\n",
    "  # dtype([('date', '<M8[D]'), ('time', '<m8[us]'), ('open_p', '<f8'), ('high_p', '<f8'), ('low_p', '<f8'), ('close_p', '<f8'), ('tot_vlm', '<u8'), ('prd_vlm', '<u8'), ('num_trds', '<u8')])\n",
    "  ndarray = histConn.request_bars_in_period(\n",
    "    ticker = symbol,\n",
    "    interval_len = 300, # 5 min bars\n",
    "    interval_type = 's',\n",
    "    bgn_prd = bgn_prd,\n",
    "    end_prd = end_prd,\n",
    "    bgn_flt = datetime.time.fromisoformat( '09:30:00' ), # None,\n",
    "    end_flt = datetime.time.fromisoformat( '16:00:00' ), # None,\n",
    "    ascend = True,\n",
    "    max_bars = None,\n",
    "    label_at_beginning = False,\n",
    "    timeout = 30\n",
    "  )\n",
    "  df = pd.DataFrame( ndarray )\n",
    "  df['datetime'] = df['date'] + df['time']\n",
    "  df.drop('date', axis=1, inplace=True)\n",
    "  df.drop('time', axis=1, inplace=True)\n",
    "  df.drop('tot_vlm', axis=1, inplace=True)\n",
    "  df.drop('num_trds', axis=1, inplace=True)\n",
    "  df.rename( columns={'open_p': 'open', 'high_p': 'high', 'low_p': 'low', 'close_p': 'close', 'prd_vlm': 'volume'}, inplace=True)\n",
    "  df.set_index( 'datetime', inplace=True )\n",
    "  #print( df.info() )\n",
    "\n",
    "  histConn.remove_listener( histListener )\n",
    "  histConn.disconnect()\n",
    "\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "30dac027-1f55-408c-82e8-3b77b9b74250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/520 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 520 ] 5-minute data loaded...\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Generate/persist and load 5-minute OHLCV data from IQFeed for all 520 symbols\n",
    "#\n",
    "ohlcv = {}\n",
    "bgn_prd = datetime.datetime( year=2020, month=1, day=1, hour=0, minute=0, second=0 )\n",
    "end_prd = datetime.datetime.now()\n",
    "\n",
    "for i in tqdm( range( len( uniqueSymbols )), leave=False ):\n",
    "  \n",
    "  symbol = uniqueSymbols[i]\n",
    "  #print( 'symbol=[', symbol, ']' )\n",
    "\n",
    "  fn_pkl = 'data/5min/' + symbol + '.pkl'\n",
    "\n",
    "  if not os.path.isfile( fn_pkl ): # only download if we don't already have the data locally\n",
    "    iqData = get_historical_data( symbol, bgn_prd, end_prd )\n",
    "    #print( iqData[:5] )\n",
    "    startTime = datetime.datetime.now()\n",
    "    iqData.to_pickle( fn_pkl )\n",
    "    deltaTime = datetime.datetime.now() - startTime\n",
    "    #print( \"iqData to_pickle(\" + fn_pkl + \") elapsed {:.3f}ms\".format( deltaTime.total_seconds() * 1000 )) # milliseconds\n",
    "\n",
    "  startTime = datetime.datetime.now()\n",
    "  ohlcv[symbol] = pd.read_pickle( fn_pkl )\n",
    "  deltaTime = datetime.datetime.now() - startTime\n",
    "  #print( len( ohlcv[symbol] ), \"ohlcv read_pickle(\" + fn_pkl + \") elapsed {:.3f}ms\".format( deltaTime.total_seconds() * 1000 )) # milliseconds\n",
    "\n",
    "print( \"[\", len( ohlcv ), \"] 5-minute data loaded...\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "39bb240d-7e67-4d10-b92e-7dddafa8844d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/520 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 520 ] hourly data loaded...\n",
      "[ 520 ] daily data loaded...\n",
      "[ 520 ] weekly data loaded...\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Generate and load hourly, daily and weekly data from 5 minute OHLCV data for all 520 symbols\n",
    "#\n",
    "ohlcvH = {}\n",
    "ohlcvD = {}\n",
    "ohlcvW = {}\n",
    "how = {\n",
    "  'open': 'first',\n",
    "  'high': 'max',\n",
    "  'low': 'min',\n",
    "  'close': 'last',\n",
    "  'volume': 'sum'\n",
    "}\n",
    "\n",
    "for i in tqdm( range( len( uniqueSymbols )), leave=False ):\n",
    "\n",
    "  symbol = uniqueSymbols[i]\n",
    "\n",
    "  fn_pkl = 'data/hourly/' + symbol + '.pkl'\n",
    "\n",
    "  if not os.path.isfile( fn_pkl ): # only download if we don't already have the data locally\n",
    "    df = ohlcv[symbol].resample( '1h', offset=0 ).apply( how ).dropna()\n",
    "    #print( ohlcv[symbol][:20] )\n",
    "    #print( df[:20] )\n",
    "    startTime = datetime.datetime.now()\n",
    "    df.to_pickle( fn_pkl )\n",
    "    deltaTime = datetime.datetime.now() - startTime\n",
    "    #print( \"hourly to_pickle(\" + fn_pkl + \") elapsed {:.3f}ms\".format( deltaTime.total_seconds() * 1000 )) # milliseconds\n",
    "\n",
    "  startTime = datetime.datetime.now()\n",
    "  ohlcvH[symbol] = pd.read_pickle( fn_pkl )\n",
    "  deltaTime = datetime.datetime.now() - startTime\n",
    "  #print( len( ohlcvH[symbol] ), \"hourly read_pickle(\" + fn_pkl + \") elapsed {:.3f}ms\".format( deltaTime.total_seconds() * 1000 )) # milliseconds\n",
    "\n",
    "  fn_pkl = 'data/daily/' + symbol + '.pkl'\n",
    "\n",
    "  if not os.path.isfile( fn_pkl ): # only download if we don't already have the data locally\n",
    "    df = ohlcv[symbol].resample( '1d', offset=0 ).apply( how ).dropna()\n",
    "    #print( ohlcv[symbol][:20] )\n",
    "    #print( df[:20] )\n",
    "    startTime = datetime.datetime.now()\n",
    "    df.to_pickle( fn_pkl )\n",
    "    deltaTime = datetime.datetime.now() - startTime\n",
    "    #print( \"hourly to_pickle(\" + fn_pkl + \") elapsed {:.3f}ms\".format( deltaTime.total_seconds() * 1000 )) # milliseconds\n",
    "\n",
    "  startTime = datetime.datetime.now()\n",
    "  ohlcvD[symbol] = pd.read_pickle( fn_pkl )\n",
    "  deltaTime = datetime.datetime.now() - startTime\n",
    "  #print( len( ohlcvH[symbol] ), \"hourly read_pickle(\" + fn_pkl + \") elapsed {:.3f}ms\".format( deltaTime.total_seconds() * 1000 )) # milliseconds\n",
    "\n",
    "  fn_pkl = 'data/weekly/' + symbol + '.pkl'\n",
    "\n",
    "  if not os.path.isfile( fn_pkl ): # only download if we don't already have the data locally\n",
    "    df = ohlcv[symbol].resample( '1w', offset=0 ).apply( how ).dropna()\n",
    "    #print( ohlcv[symbol][:20] )\n",
    "    #print( df[:20] )\n",
    "    startTime = datetime.datetime.now()\n",
    "    df.to_pickle( fn_pkl )\n",
    "    deltaTime = datetime.datetime.now() - startTime\n",
    "    #print( \"hourly to_pickle(\" + fn_pkl + \") elapsed {:.3f}ms\".format( deltaTime.total_seconds() * 1000 )) # milliseconds\n",
    "\n",
    "  startTime = datetime.datetime.now()\n",
    "  ohlcvW[symbol] = pd.read_pickle( fn_pkl )\n",
    "  deltaTime = datetime.datetime.now() - startTime\n",
    "  #print( len( ohlcvH[symbol] ), \"hourly read_pickle(\" + fn_pkl + \") elapsed {:.3f}ms\".format( deltaTime.total_seconds() * 1000 )) # milliseconds\n",
    "\n",
    "print( \"[\", len( ohlcvH ), \"] hourly data loaded...\" )\n",
    "print( \"[\", len( ohlcvD ), \"] daily data loaded...\" )\n",
    "print( \"[\", len( ohlcvW ), \"] weekly data loaded...\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f42f3ed2-c12d-4367-8207-740cb09f1767",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/520 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 520 ] Fractal/Pivot generation elapsed 19.813s\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Generate fractal/pivot reversal points\n",
    "#\n",
    "TP = 50 # TODO: ATR-based?\n",
    "startTime = datetime.datetime.now()\n",
    "\n",
    "for i in tqdm( range( len( uniqueSymbols )), leave=False ):\n",
    "\n",
    "  symbol = uniqueSymbols[i]\n",
    "  #print( 'symbol=[', symbol, ']' )\n",
    "\n",
    "  df_tmp = ohlcv[symbol][['high', 'low', 'open']].copy()\n",
    "\n",
    "  df_tmp = df_tmp.assign(fh = np.where(\n",
    "    (df_tmp['high'] > df_tmp['high'].shift(1)) &\n",
    "    (df_tmp['high'] > df_tmp['high'].shift(2)) &\n",
    "    (df_tmp['high'] > df_tmp['high'].shift(3)) &\n",
    "    (df_tmp['high'] > df_tmp['high'].shift(4)) &\n",
    "    (df_tmp['high'] > df_tmp['high'].shift(-1)) &\n",
    "    (df_tmp['high'] > df_tmp['high'].shift(-2)), # &\n",
    "    #(df_tmp['high'] > df_tmp['high'].shift(-3)),\n",
    "    1, 0\n",
    "  ))\n",
    "  df_tmp = df_tmp.assign(fl = np.where(\n",
    "    (df_tmp['low'] < df_tmp['low'].shift(1)) &\n",
    "    (df_tmp['low'] < df_tmp['low'].shift(2)) &\n",
    "    (df_tmp['low'] < df_tmp['low'].shift(3)) &\n",
    "    (df_tmp['low'] < df_tmp['low'].shift(4)) &\n",
    "    (df_tmp['low'] < df_tmp['low'].shift(-1)) &\n",
    "    (df_tmp['low'] < df_tmp['low'].shift(-2)), # &\n",
    "    #(df_tmp['low'] < df_tmp['low'].shift(-3)),\n",
    "    1, 0\n",
    "  ))\n",
    "  df_tmp = df_tmp[['fh', 'fl']]\n",
    "  ohlcv[symbol].loc[:, 'fh42'] = df_tmp['fh']\n",
    "  ohlcv[symbol].loc[:, 'fl42'] = df_tmp['fl']\n",
    "  \n",
    "  #\n",
    "  # Determine if fractal/pivot reversal points were a \"verified\" win (within the next 3 bars)\n",
    "  #\n",
    "  df_tmp = ohlcv[symbol][['high', 'low', 'open']].copy()\n",
    "  #print( \"===[ df_tmp ]===\\n\", df_tmp.head(20), sep='')\n",
    "\n",
    "  df_tmp = df_tmp.assign(fh = np.where(\n",
    "    (df_tmp['high'] > df_tmp['high'].shift(1)) &\n",
    "    (df_tmp['high'] > df_tmp['high'].shift(2)) &\n",
    "    (df_tmp['high'] > df_tmp['high'].shift(3)) &\n",
    "    (df_tmp['high'] > df_tmp['high'].shift(4)) &\n",
    "    (df_tmp['high'] > df_tmp['high'].shift(-1)) &\n",
    "    (df_tmp['high'] > df_tmp['high'].shift(-2)) &\n",
    "    (\n",
    "      (((df_tmp['open'].shift(-1) - df_tmp['low'].shift(-2)) / .01) >= TP) |\n",
    "      (((df_tmp['open'].shift(-1) - df_tmp['low'].shift(-3)) / .01) >= TP) |\n",
    "      (((df_tmp['open'].shift(-1) - df_tmp['low'].shift(-4)) / .01) >= TP) |\n",
    "      (((df_tmp['open'].shift(-1) - df_tmp['low'].shift(-5)) / .01) >= TP) |\n",
    "      (((df_tmp['open'].shift(-1) - df_tmp['low'].shift(-6)) / .01) >= TP)\n",
    "    ),\n",
    "    #(df_tmp['high'] > df_tmp['high'].shift(-3)),\n",
    "    1, 0\n",
    "  ))\n",
    "  df_tmp = df_tmp.assign(fl = np.where(\n",
    "    (df_tmp['low'] < df_tmp['low'].shift(1)) &\n",
    "    (df_tmp['low'] < df_tmp['low'].shift(2)) &\n",
    "    (df_tmp['low'] < df_tmp['low'].shift(3)) &\n",
    "    (df_tmp['low'] < df_tmp['low'].shift(4)) &\n",
    "    (df_tmp['low'] < df_tmp['low'].shift(-1)) &\n",
    "    (df_tmp['low'] < df_tmp['low'].shift(-2)) &\n",
    "    (\n",
    "      (((df_tmp['high'].shift(-2) - df_tmp['open'].shift(-1)) / .01) >= TP) |\n",
    "      (((df_tmp['high'].shift(-3) - df_tmp['open'].shift(-1)) / .01) >= TP) |\n",
    "      (((df_tmp['high'].shift(-4) - df_tmp['open'].shift(-1)) / .01) >= TP) |\n",
    "      (((df_tmp['high'].shift(-5) - df_tmp['open'].shift(-1)) / .01) >= TP) |\n",
    "      (((df_tmp['high'].shift(-6) - df_tmp['open'].shift(-1)) / .01) >= TP)\n",
    "    ),\n",
    "    #(df_tmp['low'] < df_tmp['low'].shift(-3)),\n",
    "    1, 0\n",
    "  ))\n",
    "  df_tmp = df_tmp[['fh', 'fl']]\n",
    "  ohlcv[symbol].loc[:, 'fh42v'] = df_tmp['fh']\n",
    "  ohlcv[symbol].loc[:, 'fl42v'] = df_tmp['fl']\n",
    "  #print( df_tmp.head(20) )\n",
    "\n",
    "deltaTime = datetime.datetime.now() - startTime\n",
    "\n",
    "print( \"[\", len( ohlcv ), \"] Fractal/Pivot generation elapsed {:.3f}s\".format( deltaTime.total_seconds() )) # milliseconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c9405368-789d-450b-a2e6-c9a0c125ec62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>fh42</th>\n",
       "      <th>fl42</th>\n",
       "      <th>fh42v</th>\n",
       "      <th>fl42v</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-01-02 09:35:00</th>\n",
       "      <td>296.2400</td>\n",
       "      <td>296.926</td>\n",
       "      <td>295.1900</td>\n",
       "      <td>296.87</td>\n",
       "      <td>1649209</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-02 09:40:00</th>\n",
       "      <td>296.9300</td>\n",
       "      <td>297.950</td>\n",
       "      <td>296.9000</td>\n",
       "      <td>297.27</td>\n",
       "      <td>1071400</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-02 09:45:00</th>\n",
       "      <td>297.2700</td>\n",
       "      <td>297.710</td>\n",
       "      <td>296.8700</td>\n",
       "      <td>297.12</td>\n",
       "      <td>674630</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-02 09:50:00</th>\n",
       "      <td>297.1300</td>\n",
       "      <td>297.420</td>\n",
       "      <td>296.8000</td>\n",
       "      <td>296.83</td>\n",
       "      <td>536942</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-02 09:55:00</th>\n",
       "      <td>296.8000</td>\n",
       "      <td>297.300</td>\n",
       "      <td>296.7000</td>\n",
       "      <td>297.28</td>\n",
       "      <td>434840</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-02 10:00:00</th>\n",
       "      <td>297.3000</td>\n",
       "      <td>298.150</td>\n",
       "      <td>297.2500</td>\n",
       "      <td>297.97</td>\n",
       "      <td>720271</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-02 10:05:00</th>\n",
       "      <td>297.9900</td>\n",
       "      <td>298.080</td>\n",
       "      <td>297.5100</td>\n",
       "      <td>297.80</td>\n",
       "      <td>524797</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-02 10:10:00</th>\n",
       "      <td>297.8000</td>\n",
       "      <td>298.410</td>\n",
       "      <td>297.8000</td>\n",
       "      <td>298.26</td>\n",
       "      <td>580352</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-02 10:15:00</th>\n",
       "      <td>298.2693</td>\n",
       "      <td>298.310</td>\n",
       "      <td>297.3101</td>\n",
       "      <td>297.39</td>\n",
       "      <td>556495</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-02 10:20:00</th>\n",
       "      <td>297.3701</td>\n",
       "      <td>297.610</td>\n",
       "      <td>296.9300</td>\n",
       "      <td>297.22</td>\n",
       "      <td>527967</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         open     high       low   close   volume  fh42  fl42  \\\n",
       "datetime                                                                        \n",
       "2020-01-02 09:35:00  296.2400  296.926  295.1900  296.87  1649209     0     0   \n",
       "2020-01-02 09:40:00  296.9300  297.950  296.9000  297.27  1071400     0     0   \n",
       "2020-01-02 09:45:00  297.2700  297.710  296.8700  297.12   674630     0     0   \n",
       "2020-01-02 09:50:00  297.1300  297.420  296.8000  296.83   536942     0     0   \n",
       "2020-01-02 09:55:00  296.8000  297.300  296.7000  297.28   434840     0     0   \n",
       "2020-01-02 10:00:00  297.3000  298.150  297.2500  297.97   720271     0     0   \n",
       "2020-01-02 10:05:00  297.9900  298.080  297.5100  297.80   524797     0     0   \n",
       "2020-01-02 10:10:00  297.8000  298.410  297.8000  298.26   580352     1     0   \n",
       "2020-01-02 10:15:00  298.2693  298.310  297.3101  297.39   556495     0     0   \n",
       "2020-01-02 10:20:00  297.3701  297.610  296.9300  297.22   527967     0     0   \n",
       "\n",
       "                     fh42v  fl42v  \n",
       "datetime                           \n",
       "2020-01-02 09:35:00      0      0  \n",
       "2020-01-02 09:40:00      0      0  \n",
       "2020-01-02 09:45:00      0      0  \n",
       "2020-01-02 09:50:00      0      0  \n",
       "2020-01-02 09:55:00      0      0  \n",
       "2020-01-02 10:00:00      0      0  \n",
       "2020-01-02 10:05:00      0      0  \n",
       "2020-01-02 10:10:00      1      0  \n",
       "2020-01-02 10:15:00      0      0  \n",
       "2020-01-02 10:20:00      0      0  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohlcv['AAPL'][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d7913844-5895-4747-a715-7647b3d1d51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seedCapital=[ $1,000,000,000.00 ] investmentCapitalPerSymbol=[ $1,923,076.92 ]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/520 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "seedCapital = 1000000000\n",
    "investmentCapitalPerSymbol = seedCapital / len( uniqueSymbols )\n",
    "print( 'seedCapital=[ ${:,.2f}'.format( seedCapital ), '] investmentCapitalPerSymbol=[ ${:,.2f}'.format( investmentCapitalPerSymbol ), ']' )\n",
    "\n",
    "dictPortfolio = {}\n",
    "for i in tqdm( range( len( uniqueSymbols )), leave=False ):\n",
    "\n",
    "  symbol = uniqueSymbols[i]\n",
    "\n",
    "  open2020   = ohlcv[symbol].open['2020-01-02 09:35:00']\n",
    "  close2020  = ohlcv[symbol].close['2020-12-31 16:00:00']\n",
    "  pctChg2020 = (close2020 - open2020) / open2020\n",
    "  invest2020 = math.floor( investmentCapitalPerSymbol / open2020 ) * open2020\n",
    "\n",
    "  open2021   = ohlcv[symbol].open['2021-01-04 09:35:00']\n",
    "  close2021  = ohlcv[symbol].close['2021-12-31 16:00:00']\n",
    "  pctChg2021 = (close2021 - open2021) / open2021\n",
    "  invest2021 = math.floor( investmentCapitalPerSymbol / open2021 ) * open2021\n",
    "\n",
    "  open2022   = ohlcv[symbol].open['2022-01-03 09:35:00']\n",
    "  close2022  = ohlcv[symbol].close['2021-04-14 16:00:00']\n",
    "  pctChg2022 = (close2022 - open2022) / open2022\n",
    "  invest2022 = math.floor( investmentCapitalPerSymbol / open2022 ) * open2022\n",
    "\n",
    "  dictPortfolio[symbol] = [\n",
    "    open2020, close2020, pctChg2020 * 100.0, invest2020, invest2020 * pctChg2020,\n",
    "    open2021, close2021, pctChg2021 * 100.0, invest2021, invest2021 * pctChg2021,\n",
    "    open2022, close2022, pctChg2022 * 100.0, invest2022, invest2022 * pctChg2022\n",
    "  ]\n",
    "print( end=\"\\r\" )\n",
    "#print( dictPortfolio )\n",
    "\n",
    "dfPortfolio = pd.DataFrame.from_dict(\n",
    "  dictPortfolio, orient='index',\n",
    "  columns=[\n",
    "    'open2020','close2020','pctChg2020','invest2020','PnL2020',\n",
    "    'open2021','close2021','pctChg2021','invest2021','PnL2021',\n",
    "    'open2022','close2022','pctChg2022','invest2022','PnL2022'\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "80172c6c-c85e-4105-9602-9c12d5820dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   symbol  open2020  close2020  pctChg2020\n",
      "0    ENPH     26.37     175.47      565.42\n",
      "1    MRNA     19.57     104.47      433.83\n",
      "2      ZM     68.80     337.16      390.06\n",
      "3     PDD     38.50     177.67      361.48\n",
      "4      ZS     46.87     199.71      326.09\n",
      "5    CRWD     50.03     211.82      323.39\n",
      "6    ETSY     44.71     177.84      297.76\n",
      "7    PENN     25.97      86.38      232.61\n",
      "8    SEDG     97.00     319.12      228.99\n",
      "9    DOCU     74.31     222.39      199.27\n",
      "10   MELI    576.94    1674.80      190.29\n",
      "11   DDOG     38.22      98.42      157.51\n",
      "12     JD     35.96      87.85      144.30\n",
      "13   GNRC    101.38     227.42      124.32\n",
      "14   NVDA    238.75     522.12      118.69\n",
      "15   OKTA    116.75     254.27      117.79\n",
      "16   PYPL    109.47     234.31      114.04\n",
      "17   BBWI     18.25      37.18      103.73\n",
      "18   MPWR    180.12     366.23      103.33\n",
      "19    ALB     73.50     147.51      100.69\n",
      "20    AMD     46.86      91.72       95.73\n",
      "21    FCX     13.35      26.01       94.83\n",
      "22   CDNS     70.18     136.32       94.24\n",
      "23    NOW    284.96     550.42       93.16\n",
      "24   TEAM    121.15     233.87       93.04\n",
      "25   IDXX    261.53     499.98       91.18\n",
      "26   ALGN    281.20     534.55       90.10\n",
      "27    WST    150.25     283.37       88.60\n",
      "28   ABMD    172.51     324.30       87.99\n",
      "29   SNPS    140.31     259.24       84.76\n",
      "30   CTLT     56.54     103.97       83.89\n",
      "31   MTCH     82.77     151.19       82.66\n",
      "32   MRVL     27.00      47.54       76.07\n",
      "33   POOL    212.24     372.76       75.63\n",
      "34    PWR     41.07      72.03       75.38\n",
      "35    TER     68.84     119.89       74.16\n",
      "36   AMZN   1875.00    3256.41       73.68\n",
      "37   MSCI    260.09     446.36       71.62\n",
      "38   QCOM     89.05     152.24       70.96\n",
      "39   TMUS     78.88     134.84       70.94\n",
      "40    FDX    152.42     259.61       70.33\n",
      "41   PAYC    266.92     452.21       69.42\n",
      "42   DXCM    218.56     369.79       69.19\n",
      "43   TTWO    123.53     207.92       68.32\n",
      "44   TWTR     32.31      54.14       67.56\n",
      "45   EPAM    214.12     358.04       67.21\n",
      "46   BIDU    129.49     216.24       66.99\n",
      "47   TSLA    424.50     705.21       66.13\n",
      "48   NFLX    326.10     540.71       65.81\n",
      "49   ADSK    184.25     305.38       65.74        open2021  close2021  pctChg2021\n",
      "GE        10.89      94.47      767.49\n",
      "DVN       16.00      44.04      175.25\n",
      "MRO        6.78      16.43      142.33\n",
      "FTNT     149.57     359.40      140.29\n",
      "SBNY     136.59     323.67      136.96\n",
      "MRNA     107.23     254.05      136.92\n",
      "F          8.81      20.77      135.75\n",
      "FANG      49.01     107.82      120.00\n",
      "NUE       54.18     114.14      110.67\n",
      "IT       159.89     334.58      109.26\n",
      "EXR      115.73     226.81       95.98\n",
      "SPG       85.66     159.78       86.53\n",
      "EPAM     359.01     669.07       86.37\n",
      "APA       14.56      26.89       84.68\n",
      "ODFL     195.01     358.25       83.71\n",
      "BBWI      38.05      69.80       83.44\n",
      "MRVL      47.75      87.57       83.39\n",
      "CF        39.00      70.77       81.46\n",
      "STX       62.29     112.98       81.38\n",
      "MAA      126.69     229.52       81.17\n",
      "DDOG      98.69     178.12       80.48\n",
      "AMAT      87.22     157.31       80.36\n",
      "CPT       99.78     178.74       79.13\n",
      "RHI       62.51     111.47       78.32\n",
      "COP       40.50      72.20       78.27\n",
      "AZO     1183.56    2096.12       77.10\n",
      "SIVB     385.66     678.94       76.05\n",
      "EOG       50.76      88.84       75.02\n",
      "JCI       46.61      81.31       74.45\n",
      "IRM       30.00      52.32       74.40\n",
      "CBRE      62.74     108.47       72.89\n",
      "TSCO     140.72     238.66       69.60\n",
      "INTU     379.32     643.19       69.56\n",
      "LKQ       35.44      60.02       69.36\n",
      "PLD       99.55     168.39       69.15\n",
      "MOS       23.35      39.30       68.31\n",
      "WST      283.43     468.85       65.42\n",
      "GOOG    1757.54    2893.58       64.64\n",
      "REG       45.77      75.35       64.63\n",
      "GOOGL   1760.00    2897.04       64.60\n",
      "KIM       15.02      24.66       64.18\n",
      "KLAC     261.99     430.11       64.17\n",
      "DRE       40.04      65.66       63.99\n",
      "TECH     316.66     517.86       63.54\n",
      "LLY      169.02     276.11       63.36\n",
      "OXY       17.75      28.98       63.27\n",
      "LYV       73.42     119.74       63.09\n",
      "PSA      230.32     374.64       62.66\n",
      "TEAM     234.50     381.35       62.62\n",
      "PWR       70.80     114.69       61.99\n"
     ]
    }
   ],
   "source": [
    "dfPortfolio.sort_values( by=['pctChg2020'], ascending=False, inplace=True)\n",
    "df2020Top50 = dfPortfolio[['open2020','close2020','pctChg2020']][0:50]\n",
    "\n",
    "df2020Top50.reset_index(inplace=True)\n",
    "df2020Top50 = df2020Top50.rename( columns = {'index':'symbol'})\n",
    "\n",
    "df2020Top50\n",
    "\n",
    "dfPortfolio.sort_values( by=['pctChg2021'], ascending=False, inplace=True)\n",
    "df2021Top50 = dfPortfolio[['open2021','close2021','pctChg2021']][0:50]\n",
    "\n",
    "dfPortfolio.sort_values( by=['pctChg2022'], ascending=False, inplace=True)\n",
    "df2022Top50 = dfPortfolio[['open2022','close2022','pctChg2022']][0:50]\n",
    "\n",
    "with pd.option_context( 'display.width', 1000, 'display.precision', 2 ):\n",
    "  print( df2020Top50, df2021Top50 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfed13f-474b-4868-a0c1-4cf1bbe4ccf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with pd.option_context( 'display.width', 1000, 'display.precision', 2 ):\n",
    "#  print( dfPortfolio )\n",
    "\n",
    "BnHPnL2020All = dfPortfolio['PnL2020'].sum()\n",
    "BnHPnL2021All = dfPortfolio['PnL2021'].sum()\n",
    "BnHPnL2022All = dfPortfolio['PnL2022'].sum()\n",
    "\n",
    "print( \"\\n2020 B&H PnL (All)=[ ${:,.2f}\".format( BnHPnL2020All ), \"({:.1f}%) ] \".format( (BnHPnL2020All / seedCapital) * 100.0 ))\n",
    "print( \"2021 B&H PnL (All)=[ ${:,.2f}\".format( BnHPnL2021All ), \"({:.1f}%) ] \".format( (BnHPnL2021All / seedCapital) * 100.0 ))\n",
    "print( \"YTD 2022 B&H PnL (All)=[ ${:,.2f}\".format( BnHPnL2022All ), \"({:.1f}%) ] \".format( (BnHPnL2022All / seedCapital) * 100.0 ))\n",
    "\n",
    "BnHPnL2020WinnersOnly = dfPortfolio.query(\"pctChg2020 > 0.0\")['PnL2020'].sum()\n",
    "BnHPnL2021WinnersOnly = dfPortfolio.query(\"pctChg2021 > 0.0\")['PnL2021'].sum()\n",
    "BnHPnL2022WinnersOnly = dfPortfolio.query(\"pctChg2022 > 0.0\")['PnL2022'].sum()\n",
    "\n",
    "numBnHPnL2020WinnersOnly = dfPortfolio.query(\"pctChg2020 > 0.0\")['PnL2020'].count()\n",
    "numBnHPnL2021WinnersOnly = dfPortfolio.query(\"pctChg2021 > 0.0\")['PnL2021'].count()\n",
    "numBnHPnL2022WinnersOnly = dfPortfolio.query(\"pctChg2022 > 0.0\")['PnL2022'].count()\n",
    "\n",
    "print( \"\\n2020 B&H PnL (Winners-only)=[ ${:,.2f}\".format( BnHPnL2020WinnersOnly ), \"({:.1f}%;\".format( (BnHPnL2020WinnersOnly / seedCapital) * 100.0 ), numBnHPnL2020WinnersOnly, 'of', len( uniqueSymbols ), ')]' )\n",
    "print( \"2021 B&H PnL (Winners-only)=[ ${:,.2f}\".format( BnHPnL2021WinnersOnly ), \"({:.1f}%;\".format( (BnHPnL2021WinnersOnly / seedCapital) * 100.0 ), numBnHPnL2021WinnersOnly, 'of', len( uniqueSymbols ), ')]' )\n",
    "print( \"YTD 2022 B&H PnL (Winners-only)=[ ${:,.2f}\".format( BnHPnL2022WinnersOnly ), \"({:.1f}%;\".format( (BnHPnL2022WinnersOnly / seedCapital) * 100.0 ), numBnHPnL2022WinnersOnly, 'of', len( uniqueSymbols ), ')]' )\n",
    "\n",
    "#ohlcv[] for symbol in uniqueSymbols\n",
    "sys.exit( 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817b11b5-86b8-45b5-9bab-19d41c58ad77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Refactor from here and below...\n",
    "#\n",
    "def get_roc(close, n):\n",
    "  difference = close.diff(n)\n",
    "  nprev_values = close.shift(n)\n",
    "  roc = (difference / nprev_values) * 100\n",
    "  return roc\n",
    "\n",
    "def get_kst(close, sma1, sma2, sma3, sma4, roc1, roc2, roc3, roc4, signal):\n",
    "  rcma1 = get_roc(close, roc1).rolling(sma1).mean()\n",
    "  rcma2 = get_roc(close, roc2).rolling(sma2).mean()\n",
    "  rcma3 = get_roc(close, roc3).rolling(sma3).mean()\n",
    "  rcma4 = get_roc(close, roc4).rolling(sma4).mean()\n",
    "  kst = (rcma1 * 1) + (rcma2 * 2) + (rcma3 * 3) + (rcma4 * 4)\n",
    "  signal = kst.rolling(signal).mean()\n",
    "  return kst, signal\n",
    "\n",
    "tsla['kst'], tsla['signal_line'] = get_kst(tsla['close'], 10, 10, 10, 15, 10, 15, 20, 30, 9)\n",
    "tsla = tsla[tsla.index >= '2022-01-01']\n",
    "print(tsla.tail())\n",
    "\n",
    "ax1 = plt.subplot2grid((11,1), (0,0), rowspan = 5, colspan = 1)\n",
    "ax2 = plt.subplot2grid((11,1), (6,0), rowspan = 5, colspan = 1)\n",
    "ax1.plot(tsla['close'], linewidth = 2.5)\n",
    "ax1.set_title('TSLA CLOSING PRICES')\n",
    "ax2.plot(tsla['kst'], linewidth = 2, label = 'KST', color = 'orange')\n",
    "ax2.plot(tsla['signal_line'], linewidth = 2, label = 'SIGNAL', color = 'mediumorchid')\n",
    "ax2.legend()\n",
    "ax2.set_title('TSLA KST')\n",
    "plt.show()\n",
    "\n",
    "def implement_kst_strategy(prices, kst_line, signal_line):\n",
    "  buy_price = []\n",
    "  sell_price = []\n",
    "  kst_signal = []\n",
    "  signal = 0\n",
    "    \n",
    "  for i in range(len(kst_line)):\n",
    "        \n",
    "    if kst_line[i-1] < signal_line[i-1] and kst_line[i] > signal_line[i]:\n",
    "        if signal != 1:\n",
    "          buy_price.append(prices[i])\n",
    "          sell_price.append(np.nan)\n",
    "          signal = 1\n",
    "          kst_signal.append(signal)\n",
    "        else:\n",
    "          buy_price.append(np.nan)\n",
    "          sell_price.append(np.nan)\n",
    "          kst_signal.append(0)\n",
    "                \n",
    "    elif kst_line[i-1] > signal_line[i-1] and kst_line[i] < signal_line[i]:\n",
    "      if signal != -1:\n",
    "        buy_price.append(np.nan)\n",
    "        sell_price.append(prices[i])\n",
    "        signal = -1\n",
    "        kst_signal.append(signal)\n",
    "      else:\n",
    "        buy_price.append(np.nan)\n",
    "        sell_price.append(np.nan)\n",
    "        kst_signal.append(0)\n",
    "                \n",
    "    else:\n",
    "      buy_price.append(np.nan)\n",
    "      sell_price.append(np.nan)\n",
    "      kst_signal.append(0)\n",
    "            \n",
    "  return buy_price, sell_price, kst_signal\n",
    "\n",
    "buy_price, sell_price, kst_signal = implement_kst_strategy(tsla['close'], tsla['kst'], tsla['signal_line'])\n",
    "\n",
    "ax1 = plt.subplot2grid((11,1), (0,0), rowspan = 5, colspan = 1)\n",
    "ax2 = plt.subplot2grid((11,1), (6,0), rowspan = 5, colspan = 1)\n",
    "ax1.plot(tsla['close'], linewidth = 2, label = 'TSLA')\n",
    "ax1.plot(tsla.index, buy_price, marker = '^', markersize = 12, linewidth = 0, color = 'green', label = 'BUY SIGNAL')\n",
    "ax1.plot(tsla.index, sell_price, marker = 'v', markersize = 12, linewidth = 0, color = 'r', label = 'SELL SIGNAL')\n",
    "ax1.legend()\n",
    "ax1.set_title('TSLA KST TRADING SIGNALS')\n",
    "ax2.plot(tsla['kst'], linewidth = 2, label = 'KST', color = 'orange')\n",
    "ax2.plot(tsla['signal_line'], linewidth = 2, label = 'SIGNAL', color = 'mediumorchid')\n",
    "ax2.legend()\n",
    "ax2.set_title('TSLA KST')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311d2f2c-f666-4842-a196-529dfeddfcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Bokeh plot(s)\n",
    "#\n",
    "from math import pi\n",
    "import bokeh\n",
    "from bokeh.plotting import figure, ColumnDataSource\n",
    "from bokeh.io import output_notebook, show, curdoc\n",
    "\n",
    "from bokeh.models import BooleanFilter, CDSView, Select, Range1d\n",
    "from bokeh.models import Span, CrosshairTool, HoverTool, ResetTool, PanTool, WheelZoomTool\n",
    "from bokeh.models.formatters import NumeralTickFormatter, DatetimeTickFormatter\n",
    "from bokeh.models.widgets import Dropdown\n",
    "\n",
    "from bokeh.layouts import column\n",
    "from bokeh.palettes import Category20\n",
    "from bokeh.resources import INLINE\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "RED        = Category20[7][6]\n",
    "DARKRED    = '#d00000'\n",
    "#GREEN      = Category20[5][4]\n",
    "GREEN      = '#00ff00'\n",
    "DARKGREEN  = '#008000'\n",
    "#BLUE       = Category20[3][0]\n",
    "BLUE       = '#0000ff'\n",
    "BLUE_LIGHT = Category20[3][1]\n",
    "#ORANGE     = Category20[3][2]\n",
    "ORANGE     = '#ff9900'\n",
    "#PURPLE     = Category20[9][8]\n",
    "PURPLE     = '#7b7bc0'\n",
    "BROWN      = Category20[11][10]\n",
    "WHITE      = '#ffffff'\n",
    "GRAY       = '#505050'\n",
    "YELLOW     = '#ffff00'\n",
    "CYAN       = '#00ffff'\n",
    "MAGENTA    = '#ff00ff'\n",
    "\n",
    "chart_params = {\n",
    "  'title' : symbol,\n",
    "  'colors': {'up':'Green', 'down': 'Red'},\n",
    "  'size'  : {'height': 500 , 'width': 1000} #,\n",
    "  #'days'  : 100\n",
    "}\n",
    "\n",
    "VBAR_WIDTH = .5\n",
    "\n",
    "def BokehChart( price_data ):\n",
    "\n",
    "  numBars = 10000 if price_data.shape[0] > 10000 else price_data.shape[0]\n",
    "\n",
    "  chart_data = price_data[0:numBars]\n",
    "  chart_data = chart_data.assign( bar = np.arange( 1, len( chart_data ) + 1))\n",
    "  #chart_data['bar'] += 1\n",
    "\n",
    "  cds = ColumnDataSource( chart_data )\n",
    "  #print( 'cds=[', cds.data, ']' )\n",
    "  #cds = ColumnDataSource( data=dict( Date=[], Open=[], High=[], Low=[], Close=[], index=[] ))\n",
    "  #cds.data = cds.from_df( price_data )\n",
    "  #elements = list()\n",
    "\n",
    "  # Bokeh comes with a list of tools that include xpan and crosshair.\n",
    "  # Where pan allows you to move the chart in the y and x axis, the xpan limits this movement to the x-axis.\n",
    "  #TOOLS = 'xpan,crosshair,wheel_zoom,hover,reset'\n",
    "  TOOLS = [\n",
    "    CrosshairTool(line_color='white'),\n",
    "    PanTool(dimensions='width'),\n",
    "    #HoverTool(tooltips=[(\"Time\", \"@Date_Time{%F}\"), ('O', '@open{0.00}'), ('H', '@high{0.00}'), ('L', '@low{0.00}'), ('C', '@close{0.00}')], formatters={\"@Time\": \"datetime\"}, mode='vline'),\n",
    "    WheelZoomTool(dimensions='width'),\n",
    "    ResetTool()\n",
    "  ]\n",
    "\n",
    "  # Select specific tool for the plot\n",
    "  #price_hover = p.select( dict( type=HoverTool ))\n",
    "\n",
    "  # Choose, which glyphs are active by glyph name\n",
    "  #price_hover.names = [\"price\"]\n",
    "\n",
    "  # Creating tooltips\n",
    "  #price_hover.tooltips = [(\"Datetime\", \"@Date{%Y-%m-%d}\"),\n",
    "  #                        (\"Open\", \"@Open{$0,0.00}\"),\n",
    "  #                        (\"Close\", \"@Close{$0,0.00}\"),\n",
    "  #                       (\"Volume\", \"@Volume{($ 0.00 a)}\")]\n",
    "  #price_hover.formatters={\"Date\": 'datetime'}\n",
    "\n",
    "  p = figure(\n",
    "    tools=TOOLS, toolbar_location = 'above',\n",
    "    plot_width = chart_params['size']['width'], plot_height = chart_params['size']['height'],\n",
    "    title = chart_params['title'],\n",
    "    x_range = (1, 100),\n",
    "    y_axis_location = 'right'\n",
    "  )\n",
    "  #Range1d(bounds=(0, 1000))\n",
    "  #x_axis_type = 'linear',\n",
    "\n",
    "  p.background_fill_color = \"black\"\n",
    "  #p.xaxis.major_label_orientation = pi / 4\n",
    "  p.xaxis.ticker.desired_num_ticks = 5\n",
    "  p.xaxis.major_label_overrides = {i: '{:02d}:{:02d}:{:02d}'.format( chart_data.index[i].hour, chart_data.index[i].minute, chart_data.index[i].second ) for i in range( numBars )}\n",
    "  #p.xaxis.formatter=DatetimeTickFormatter( # \"%d %B %Y\"\n",
    "  #  minutes=[\"%H:%M:%S\"],\n",
    "  #  hours=[\"%H:%M:%S\"],\n",
    "  #  days=[\"%H:%M:%S\"],\n",
    "  #  months=[\"%H:%M:%S\"],\n",
    "  #  years=[\"%H:%M:%S\"]\n",
    "  #)\n",
    "  #ticker = SingleIntervalTicker(interval=5, num_minor_ticks=10)\n",
    "  #xaxis = LinearAxis(ticker=ticker)\n",
    "  #p.add_layout(xaxis, 'below')\n",
    "  p.grid.grid_line_dash = [1, 3]\n",
    "  p.grid.grid_line_alpha = 0.4\n",
    "\n",
    "  #p.circle( x='bar', y='vwap', size=3, fill_color=CYAN, line_color=CYAN, source=cds)\n",
    "\n",
    "  ###\n",
    "  p.yaxis.axis_label_text_font_size = \"12pt\"\n",
    "  #p.yaxis.major_label_orientation = 'vertical'\n",
    "\n",
    "  # map dataframe indices to date strings and use as label overrides\n",
    "  #p.xaxis.major_label_overrides = {\n",
    "  #  i+int(chart_data['index'][0]): date.strftime('%b %d') for i, date in enumerate(pd.to_datetime(chart_data[\"Date_Time\"]))\n",
    "  #}\n",
    "  #p.xaxis.bounds = (chart_data['index'][0], chart_data['index'][-1])\n",
    "\n",
    "  # Add more ticks in the plot\n",
    "  #p.x_range.range_padding = 0.05\n",
    "  #p.xaxis.ticker.desired_num_ticks = 40\n",
    "  #p.xaxis.major_label_orientation = 3.14/4\n",
    "  ###\n",
    "\n",
    "  #\n",
    "  # Price bars (x=CurrentBar, y=price)\n",
    "  #\n",
    "  mids = (chart_data.Open + chart_data.Close) / 2\n",
    "  spans = abs( chart_data.Close - chart_data.Open )\n",
    "  inc = chart_data.Close > chart_data.Open\n",
    "  dec = chart_data.Open >= chart_data.Close\n",
    "  view_inc = CDSView( source=cds, filters=[BooleanFilter( inc )] )\n",
    "  view_dec = CDSView( source=cds, filters=[BooleanFilter( dec )] )\n",
    "\n",
    "  #p.segment( x0='Date_Time', x1='Date_Time', y0='low', y1='high', color='white', source=cds, view=view_inc )\n",
    "  #p.segment( x0='Date_Time', x1='Date_Time', y0='low', y1='high', color='white', source=cds, view=view_dec )\n",
    "  #p.vbar( x='Date_Time', line_width=VBAR_WIDTH, top='open', bottom='close', fill_color='white', line_color='white', source=cds, view=view_inc, name=\"price\")\n",
    "  #p.vbar( x='Date_Time', line_width=VBAR_WIDTH, top='open', bottom='close', fill_color=ORANGE, line_color=ORANGE, source=cds, view=view_dec, name=\"price\")\n",
    "\n",
    "  p.segment( x0='bar', x1='bar', y0='Low', y1='High', color='white', source=cds, view=view_inc )\n",
    "  p.segment( x0='bar', x1='bar', y0='Low', y1='High', color='white', source=cds, view=view_dec )\n",
    "  p.vbar( x='bar', width=VBAR_WIDTH, top='Open', bottom='Close', fill_color='white', line_color='white', source=cds, view=view_inc, name=\"price\")\n",
    "  p.vbar( x='bar', width=VBAR_WIDTH, top='Open', bottom='Close', fill_color=ORANGE, line_color=ORANGE, source=cds, view=view_dec, name=\"price\")\n",
    "\n",
    "  return p, chart_data, cds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c82bd12-e91a-4d77-a796-ff4b6d0e9682",
   "metadata": {},
   "outputs": [],
   "source": [
    "(p, chart_data, cds) = BokehChart( df )\n",
    "#BokehDonchian( p, cds )\n",
    "#BokehReversals( p, chart_data )\n",
    "#BokehFractals( p, chart_data )\n",
    "#pADX = BokehADX( p, cds )\n",
    "#pMFI = BokehMFI( p, cds )\n",
    "#pOBV = BokehOBV( p, cds )\n",
    "#pPPO = BokehPPO( p, cds )\n",
    "#pSMI = BokehSMI( p, cds )\n",
    "#pVelocity = BokehVelocity( p, chart_data )\n",
    "\n",
    "#c = column( children=[p, pADX, pSMI, pPPO, pOBV, pVelocity], spacing=0 )\n",
    "c = column( children=[p], spacing=0 )\n",
    "bokeh.io.showing.show( c )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12924aa-5b67-4e1a-a165-a1183e6f5a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a variable n with a value of 10\n",
    "n = 8\n",
    "\n",
    "dfPPO = TA.PPO( df, 3, 10, 16, column=\"Open\" )\n",
    "df.loc[:, 'PPO'] = dfPPO['PPO'].values\n",
    "df.loc[:, 'PPOAvg'] = dfPPO['SIGNAL'].values\n",
    "df.loc[:, 'PPODiff'] = dfPPO['HISTO'].values\n",
    "\n",
    "#df['RSI'] = ta.RSI(np.array(df['Close'].shift(1)), timeperiod=n)\n",
    "df['RSI'] = ta.RSI(np.array(df['Open'].shift(1)), timeperiod=n)\n",
    "\n",
    "#df['SMA'] = df['Close'].shift(1).rolling(window=n).mean()\n",
    "df['SMA'] = df['Open'].shift(1).rolling(window=n).mean()\n",
    "\n",
    "#df['Corr'] = df['Close'].shift(1).rolling(window=n).corr(df['SMA'].shift(1))\n",
    "df['Corr'] = df['Open'].shift(1).rolling(window=n).corr(df['SMA'].shift(1))\n",
    "\n",
    "df['SAR'] = ta.SAR(np.array(df['High'].shift( 1 )), np.array(df['Low'].shift( 1 )), 0.2, 0.2)\n",
    "\n",
    "df['ADX'] = ta.ADX(np.array(df['High'].shift( 1 )),\n",
    "                   np.array(df['Low'].shift( 1 )),\n",
    "                   np.array(df['Open']),\n",
    "                   timeperiod=n)\n",
    "\n",
    "df['Prev_High'] = df['High'].shift(1)\n",
    "df['Prev_Low'] = df['Low'].shift(1)\n",
    "df['Prev_Close'] = df['Close'].shift(1)\n",
    "\n",
    "# Create columns 'OO' with the difference between the current minute's open and last minute's open\n",
    "df['OO'] = df['Open'] - df['Open'].shift(1)\n",
    "\n",
    "# Create columns 'OC' with the difference between the current minute's open and last minute's close\n",
    "df['OC'] = df['Open'] - df['Prev_Close']\n",
    "\n",
    "# Create a column 'Ret' with the calculation of returns\n",
    "df['Ret'] = (df['Open'].shift(-1) - df['Open']) / df['Open']\n",
    "\n",
    "# Create n columns and assign\n",
    "for i in range(1, n): # n\n",
    "  df['return%i' % i] = df['Ret'].shift(i)\n",
    "    \n",
    "# Change the value of 'Corr' to -1 if it is less than -1\n",
    "df.loc[df['Corr'] < -1, 'Corr'] = -1\n",
    "\n",
    "# Change the value of 'Corr' to 1 if it is greater than 1\n",
    "df.loc[df['Corr'] > 1, 'Corr'] = 1\n",
    "\n",
    "# Drop the NaN values\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c60af3-5bf3-49ca-b048-d4c7911287e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_features(df):\n",
    "    open = df['op']\n",
    "    high = df['hi']\n",
    "    low = df['lo']\n",
    "    close = df['cl']\n",
    "    volume = df['volume']\n",
    "    \n",
    "    orig_columns = df.columns\n",
    "\n",
    "    hilo = (df['hi'] + df['lo']) / 2\n",
    "    df['BBANDS_upperband'], df['BBANDS_middleband'], df['BBANDS_lowerband'] = talib.BBANDS(close, timeperiod=5, nbdevup=2, nbdevdn=2, matype=0)\n",
    "    df['BBANDS_upperband'] -= hilo\n",
    "    df['BBANDS_middleband'] -= hilo\n",
    "    df['BBANDS_lowerband'] -= hilo\n",
    "    df['DEMA'] = talib.DEMA(close, timeperiod=30) - hilo\n",
    "    df['EMA'] = talib.EMA(close, timeperiod=30) - hilo\n",
    "    df['HT_TRENDLINE'] = talib.HT_TRENDLINE(close) - hilo\n",
    "    df['KAMA'] = talib.KAMA(close, timeperiod=30) - hilo\n",
    "    df['MA'] = talib.MA(close, timeperiod=30, matype=0) - hilo\n",
    "    df['MIDPOINT'] = talib.MIDPOINT(close, timeperiod=14) - hilo\n",
    "    df['SMA'] = talib.SMA(close, timeperiod=30) - hilo\n",
    "    df['T3'] = talib.T3(close, timeperiod=5, vfactor=0) - hilo\n",
    "    df['TEMA'] = talib.TEMA(close, timeperiod=30) - hilo\n",
    "    df['TRIMA'] = talib.TRIMA(close, timeperiod=30) - hilo\n",
    "    df['WMA'] = talib.WMA(close, timeperiod=30) - hilo\n",
    "\n",
    "    df['ADX'] = talib.ADX(high, low, close, timeperiod=14)\n",
    "    df['ADXR'] = talib.ADXR(high, low, close, timeperiod=14)\n",
    "    df['APO'] = talib.APO(close, fastperiod=12, slowperiod=26, matype=0)\n",
    "    df['AROON_aroondown'], df['AROON_aroonup'] = talib.AROON(high, low, timeperiod=14)\n",
    "    df['AROONOSC'] = talib.AROONOSC(high, low, timeperiod=14)\n",
    "    df['BOP'] = talib.BOP(open, high, low, close)\n",
    "    df['CCI'] = talib.CCI(high, low, close, timeperiod=14)\n",
    "    df['DX'] = talib.DX(high, low, close, timeperiod=14)\n",
    "    df['MACD_macd'], df['MACD_macdsignal'], df['MACD_macdhist'] = talib.MACD(close, fastperiod=12, slowperiod=26, signalperiod=9)\n",
    "    # skip MACDEXT MACDFIX \n",
    "    df['MFI'] = talib.MFI(high, low, close, volume, timeperiod=14)\n",
    "    df['MINUS_DI'] = talib.MINUS_DI(high, low, close, timeperiod=14)\n",
    "    df['MINUS_DM'] = talib.MINUS_DM(high, low, timeperiod=14)\n",
    "    df['MOM'] = talib.MOM(close, timeperiod=10)\n",
    "    df['PLUS_DI'] = talib.PLUS_DI(high, low, close, timeperiod=14)\n",
    "    df['PLUS_DM'] = talib.PLUS_DM(high, low, timeperiod=14)\n",
    "    df['RSI'] = talib.RSI(close, timeperiod=14)\n",
    "    df['STOCH_slowk'], df['STOCH_slowd'] = talib.STOCH(high, low, close, fastk_period=5, slowk_period=3, slowk_matype=0, slowd_period=3, slowd_matype=0)\n",
    "    df['STOCHF_fastk'], df['STOCHF_fastd'] = talib.STOCHF(high, low, close, fastk_period=5, fastd_period=3, fastd_matype=0)\n",
    "    df['STOCHRSI_fastk'], df['STOCHRSI_fastd'] = talib.STOCHRSI(close, timeperiod=14, fastk_period=5, fastd_period=3, fastd_matype=0)\n",
    "    df['TRIX'] = talib.TRIX(close, timeperiod=30)\n",
    "    df['ULTOSC'] = talib.ULTOSC(high, low, close, timeperiod1=7, timeperiod2=14, timeperiod3=28)\n",
    "    df['WILLR'] = talib.WILLR(high, low, close, timeperiod=14)\n",
    "\n",
    "    df['AD'] = talib.AD(high, low, close, volume)\n",
    "    df['ADOSC'] = talib.ADOSC(high, low, close, volume, fastperiod=3, slowperiod=10)\n",
    "    df['OBV'] = talib.OBV(close, volume)\n",
    "\n",
    "    df['ATR'] = talib.ATR(high, low, close, timeperiod=14)\n",
    "    df['NATR'] = talib.NATR(high, low, close, timeperiod=14)\n",
    "    df['TRANGE'] = talib.TRANGE(high, low, close)\n",
    "\n",
    "    df['HT_DCPERIOD'] = talib.HT_DCPERIOD(close)\n",
    "    df['HT_DCPHASE'] = talib.HT_DCPHASE(close)\n",
    "    df['HT_PHASOR_inphase'], df['HT_PHASOR_quadrature'] = talib.HT_PHASOR(close)\n",
    "    df['HT_SINE_sine'], df['HT_SINE_leadsine'] = talib.HT_SINE(close)\n",
    "    df['HT_TRENDMODE'] = talib.HT_TRENDMODE(close)\n",
    "\n",
    "    df['BETA'] = talib.BETA(high, low, timeperiod=5)\n",
    "    df['CORREL'] = talib.CORREL(high, low, timeperiod=30)\n",
    "    df['LINEARREG'] = talib.LINEARREG(close, timeperiod=14) - close\n",
    "    df['LINEARREG_ANGLE'] = talib.LINEARREG_ANGLE(close, timeperiod=14)\n",
    "    df['LINEARREG_INTERCEPT'] = talib.LINEARREG_INTERCEPT(close, timeperiod=14) - close\n",
    "    df['LINEARREG_SLOPE'] = talib.LINEARREG_SLOPE(close, timeperiod=14)\n",
    "    df['STDDEV'] = talib.STDDEV(close, timeperiod=5, nbdev=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "df = pd.read_pickle('df_ohlcv_with_fee.pkl')\n",
    "df = df.dropna()\n",
    "df = calc_features(df)\n",
    "display(df)\n",
    "df.to_pickle('df_features.pkl')\n",
    "\n",
    "def stock_features():\n",
    "  Stock_Data[i]['High Shifted'] = Stock_Data[i]['High'].shift(1)\n",
    "  Stock_Data[i]['Low Shifted'] = Stock_Data[i]['Low'].shift(1)\n",
    "  Stock_Data[i]['Close Shifted'] = Stock_Data[i]['Close'].shift(1)\n",
    "  Stock_Data[i]['Upper BBand'], Stock_Data[i]['Middle BBand'],Stock_Data[i]['Lower BBand']= ta.BBANDS(Stock_Data[i]['Close Shifted'], timeperiod=20,)\n",
    "  Stock_Data[i]['RSI'] = ta.RSI(np.array(Stock_Data[i]['Close Shifted']), timeperiod=14)\n",
    "  Stock_Data[i]['Macd'], Stock_Data[i]['Macd Signal'],Stock_Data[i]['Macd Hist'] = ta.MACD(Stock_Data[i]['Close Shifted'], fastperiod=12, slowperiod=26, signalperiod=9)\n",
    "  Stock_Data[i]['Momentum'] = ta.MOM(Stock_Data[i]['Close Shifted'],timeperiod=12)    \n",
    "  Stock_Data[i]['Returns'] = np.log(Stock_Data[i]['Open']/Stock_Data[i]['Open'].shift(1)) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c08388c-2fdc-40b9-a2c3-7742b40df928",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = .8\n",
    "split = int(t*len(df))\n",
    "print( len(df), split )\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "df['Signal'] = 0\n",
    "\n",
    "# Assign a value of 1 to 'Signal' column for the quantile with the highest returns\n",
    "df.loc[df['Ret'] > df['Ret'][:split].quantile( q=0.66 ), 'Signal'] = 1\n",
    "\n",
    "# Assign a value of -1 to 'Signal' column for the quantile with the lowest returns\n",
    "df.loc[df['Ret'] < df['Ret'][:split].quantile( q=0.34 ), 'Signal'] = -1\n",
    "\n",
    "print( 'split=[', split, ']' )\n",
    "df[-5:]\n",
    "print( len( df['Ret'][:split] ))\n",
    "#df['Ret']\n",
    "#print( df['Ret'][:split].quantile( q=0.66 ))\n",
    "#print( df['Ret'][:split].quantile( q=0.34 ))\n",
    "\n",
    "X = df.copy(deep=True)\n",
    "\n",
    "# Use drop method to drop the columns\n",
    "#X = df.drop(['Close', 'High', 'Low', 'Volume', 'Ret', 'Signal'], axis=1)\n",
    "#X = df.drop(['Close', 'Low', 'High', 'Volume', 'Ret', 'Ret1', 'Signal', 'Pred_Signal'], axis=1)\n",
    "#X = df.drop(['High', 'Low', 'Volume', 'Ret', 'Signal'], axis=1)\n",
    "#X = df.drop(['Close', 'High', 'Low', 'Volume', 'RSI', 'SMA', 'SAR', 'ADX', 'Prev_High', 'Prev_Low', 'Prev_Close', 'Ret', 'Signal', 'return1'], axis=1)\n",
    "#X = df.drop(['Open', 'High', 'Low', 'Close', 'Volume', 'Prev_High', 'Prev_Low', 'Prev_Close', 'OO', 'OC', 'Ret', 'Signal'], axis=1)\n",
    "#X[0:5]\n",
    "dropCols = ['Close', 'Low', 'High', 'Volume', 'Ret', 'Ret1', 'Signal', 'Pred_Signal']\n",
    "for col in dropCols:\n",
    "  if col in X:\n",
    "    X.drop([col], axis=1, inplace=True)\n",
    "\n",
    "print( df[0:5] )\n",
    "print( X[0:5] )\n",
    "\n",
    "y = df['Signal']\n",
    "\n",
    "if False:\n",
    "  X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.3, random_state=0 )\n",
    "\n",
    "  print( '#X_train=[', len( X_train ), ']' )\n",
    "  print( '#X_test=[', len( X_test ), ']' )\n",
    "  print( '#y_train=[', len( y_train ), ']' )\n",
    "  print( '#y_test=[', len( y_test ), ']' )\n",
    "\n",
    "  #X_train.info()\n",
    "  print( X_train[0:5] )\n",
    "  print( y_train[0:5] )\n",
    "\n",
    "print( X[0:5] )\n",
    "print( y[0:5] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c034039-3745-44d1-bf4e-c193b2b67d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "  pipeline_lr = Pipeline([('scalar1', StandardScaler()),\n",
    "                          ('pca1', PCA( n_components=2 )),\n",
    "                          ('lr_classifier', LogisticRegression( random_state=0 ))])\n",
    "  pipeline_dt = Pipeline([('scalar2', StandardScaler()),\n",
    "                          ('pca2', PCA( n_components=2)),\n",
    "                          ('dt_classifier', DecisionTreeClassifier())])\n",
    "  pipeline_rf = Pipeline([('scaler3', StandardScaler()),\n",
    "                          ('pca3', PCA( n_components=2)),\n",
    "                          ('rf_classifier', RandomForestClassifier())])\n",
    "\n",
    "  pipelines = [pipeline_lr, pipeline_dt, pipeline_rf]\n",
    "\n",
    "  best_accuracy = 0.0\n",
    "  best_classifier = 0\n",
    "  best_pipeline = \"\"\n",
    "\n",
    "  pipe_dict = {0:'Logistic Regression', 1:'Decision Tree', 2:'Random Forest'}\n",
    "\n",
    "  for pipe in pipelines:\n",
    "    pipe.fit( X_train, y_train )\n",
    "\n",
    "  for i, model in enumerate( pipelines ):\n",
    "    print( '{} test accuracy: {}'.format( pipe_dict[i], model.score( X_test, y_test )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadc6e6d-a2e0-4f62-a7cd-8a22fce54688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R\n",
    "\n",
    "# A great quantitative trading resource\n",
    "#install.packages( 'quantmod' )\n",
    "#library( quantmod )\n",
    "\n",
    "# The library containing our SVM\n",
    "#install.packages(e1071)\n",
    "#library(e1071)\n",
    "\n",
    "# The plotting tools we will use\n",
    "#install.packages(ggplot2)\n",
    "#library(ggplot2)\n",
    "\n",
    "# Our 4-hour bars of the Australian Dollar/US Dollar currency pair dating back to 01/01/2010. You can download it here for your own use.\n",
    "#Data <- AUD/USD\n",
    "\n",
    "# The 3-period relative strength index calculated off the open\n",
    "#RSI3 <- RSI(Op( Data ), n=3 )\n",
    "n = 3\n",
    "df['RSI3'] = ta.RSI(np.array( df['Open'] ), timeperiod=n)\n",
    "\n",
    "# Our measure of trend: the difference between the open price and the 50-period simple moving average.\n",
    "#SMA50 <- SMA( Op( Data ), n=50 )\n",
    "df['SMA50'] = df['Open'].rolling( window=50 ).mean()\n",
    "#Trend <- Op( Data ) - SMA50\n",
    "df['Trend'] = df['Open'] - df['SMA50']\n",
    "\n",
    "# The variable we are looking to predict, the direction of the next bar\n",
    "#Price <- Cl( Data ) - Op( Data )\n",
    "df['Price'] = df['Close'] - df['Open']\n",
    "#Class <- ifelse( Price > 0, \"UP\", \"DOWN\" )\n",
    "df['Class'] = np.where( df['Price'] >= 0, 1, -1 )\n",
    "\n",
    "# Create the data set and removing the points where our indicators are still being calculated\n",
    "#DataSet <- data.frame( RSI3, Trend, Class )\n",
    "#DataSet <- DataSet[-c(1:49),]\n",
    "df = df.dropna()\n",
    "print( df[0:5] )\n",
    "\n",
    "# Separate the data into 60% training set to build our model, 20% test set to test the patterns we found, and 20% validation set to run our strategy over new data\n",
    "#Training <- DataSet[1:4528,]\n",
    "#Test <- DataSet[4529:6038,]\n",
    "#Val <- DataSet[6039:7548,]\n",
    "\n",
    "X = df.copy( deep=True )\n",
    "\n",
    "# Use drop method to drop the columns\n",
    "dropCols = ['Low', 'High', 'Close', 'Volume', 'Price', 'Class']\n",
    "for col in dropCols:\n",
    "  if col in X:\n",
    "    X.drop([col], axis=1, inplace=True)\n",
    "\n",
    "y = df['Class']\n",
    "\n",
    "if True:\n",
    "  X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.3, random_state=None, shuffle=False )\n",
    "\n",
    "  print( '#X_train=[', len( X_train ), ']' )\n",
    "  print( '#y_train=[', len( y_train ), ']' )\n",
    "  print( '#X_test=[', len( X_test ), ']' )\n",
    "  print( '#y_test=[', len( y_test ), ']' )\n",
    "  split = len( X_train )\n",
    "\n",
    "  #X_train.info()\n",
    "  print( X_train[0:5] )\n",
    "  print( y_train[0:5] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1af9c7-ac20-4335-95ce-3a48f08095a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test variables for 'c' and 'g'\n",
    "#c = [10, 100, 1000, 2000, 3000, 4000, 5000, 10000]\n",
    "c = [100, 200, 300, 400, 500, 1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000]\n",
    "g = [1e-3, 1e-2, 1e-1, 1e0]\n",
    "\n",
    "# Intialise the parameters\n",
    "parameters = {\n",
    "  'svc__C': c,\n",
    "  'svc__gamma': g,\n",
    "  'svc__kernel': ['rbf'], # 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'\n",
    "  #'max_depth': [6, 9, None],\n",
    "  #'n_estimators': [50, 70, 100, 150],\n",
    "  #'max_features': [ random.randint( 1, 6 )],\n",
    "  #'criterion': ['gini', 'entropy'],\n",
    "  #'bootstrap': [True, False],\n",
    "  #'mln_samples_leaf': [ random.randint( 1, 4 )]\n",
    "}\n",
    "\n",
    "# Create the 'steps' variable with the pipeline functions\n",
    "steps = [('scaler', StandardScaler()), ('svc', SVC())]\n",
    "\n",
    "# Pass the 'steps' to the Pipeline function\n",
    "pipeline = Pipeline( steps )\n",
    "\n",
    "# Call the RandomizedSearchCV function and pass the parameters\n",
    "startTime = datetime.datetime.now()\n",
    "#rcv = RandomizedSearchCV( pipeline, parameters, cv=TimeSeriesSplit( n_splits=2 ), n_jobs=16 )\n",
    "rcv = GridSearchCV( pipeline, parameters, cv=TimeSeriesSplit( n_splits=2 ), n_jobs=16 )\n",
    "deltaTime = datetime.datetime.now() - startTime\n",
    "#print( \"RandomizedSearchCV elapsed {:.3f}s\".format( deltaTime.total_seconds() )) # milliseconds\n",
    "print( \"GridSearchCV elapsed {:.3f}s\".format( deltaTime.total_seconds() )) # milliseconds\n",
    "\n",
    "#X_train = X.iloc[:split]\n",
    "#y_train = y.iloc[:split]\n",
    "\n",
    "print( '#X_train=[', len( X_train ), ']' )\n",
    "print( '#y_train=[', len( y_train ), ']' )\n",
    "\n",
    "#print( '#X_test=[', len( X_test ), ']' )\n",
    "#print( '#y_test=[', len( y_test ), ']' )\n",
    "\n",
    "# Call the 'fit' method of rcv and pass the train data to it\n",
    "startTime = datetime.datetime.now()\n",
    "rcv.fit( X_train, y_train )\n",
    "deltaTime = datetime.datetime.now() - startTime\n",
    "print( \"rcv.fit( X_train, y_train ) elapsed {:.3f}s\\n\".format( deltaTime.total_seconds() )) # milliseconds\n",
    "\n",
    "startTime = datetime.datetime.now()\n",
    "\n",
    "# Call the 'best_params_' method to obtain the best parameters of C\n",
    "best_C = rcv.best_params_['svc__C']\n",
    "\n",
    "# Call the 'best_params_' method to obtain the best parameters of kernel\n",
    "best_kernel = rcv.best_params_['svc__kernel']\n",
    "\n",
    "# Call the 'best_params_' method to obtain the best parameters of gamma\n",
    "best_gamma = rcv.best_params_['svc__gamma']\n",
    "\n",
    "print( 'best_C=[', best_C, ']' )\n",
    "print( 'best_kernel=[', best_kernel, ']' )\n",
    "print( 'best_gamma=[', best_gamma, ']' )\n",
    "\n",
    "# Create a new SVC classifier\n",
    "clf = OneVsRestClassifier(\n",
    "  SVC(\n",
    "    C = best_C, # 3\n",
    "    kernel = best_kernel,\n",
    "    gamma = best_gamma,\n",
    "    verbose= True\n",
    "    # cache_size = 200,\n",
    "    # class_weight = None,\n",
    "    # coef0 = 0.0, # 3.0\n",
    "    # decision_function_shape = None,\n",
    "    # degree = 3, # 4\n",
    "    # max_iter = -1,\n",
    "    # probability = False,\n",
    "    # random_state = None,\n",
    "    # shinking = True,\n",
    "    # tol = 0.001\n",
    "  ), n_jobs=16\n",
    ")\n",
    "\n",
    "cls = SVC(C=3.0, cache_size=100, class_weight=None, coef0=3.0,\n",
    "          decision_function_shape=None, degree=4, \n",
    "          gamma='auto', kernel='rbf', max_iter=-1, probability=False,\n",
    "          random_state=None, shrinking=True, \n",
    "          tol=0.001, verbose=False,  )\n",
    "\n",
    "deltaTime = datetime.datetime.now() - startTime\n",
    "print( \"OneVsRestClassifier elapsed {:.3f}s\\n\".format( deltaTime.total_seconds() )) # milliseconds\n",
    "\n",
    "startTime = datetime.datetime.now()\n",
    "\n",
    "# Instantiate the StandardScaler\n",
    "ss1 = StandardScaler()\n",
    "\n",
    "# Pass the scaled train data to the SVC classifier\n",
    "clf.fit( ss1.fit_transform( X_train ), y_train ) # X = max_abs_scaler.fit_transform( X )\n",
    "\n",
    "#X_test = X.iloc[split:]\n",
    "print( '#X_test=[', len( X_test ), ']' )\n",
    "print( X_test[0:20] )\n",
    "\n",
    "# Pass the test data to the predict function and store the values into 'y_predict'\n",
    "y_predict = clf.predict( ss1.transform( X_test ))\n",
    "print( '#y_predict=[', len( y_predict ), ']' )\n",
    "print( y_predict[0:20] )\n",
    "\n",
    "if False:\n",
    "  # Initiate a column by name, 'Pred_Signal' and assign 0 to it\n",
    "  df[ 'Pred_Signal' ] = 0\n",
    "\n",
    "  # Save the predicted values for the train data\n",
    "  df.iloc[:split, df.columns.get_loc( 'Pred_Signal' )] = pd.Series( clf.predict( ss1.transform( X_train )).tolist() )\n",
    "\n",
    "  # Save the predicted values for the test data\n",
    "  df.iloc[split:, df.columns.get_loc( 'Pred_Signal' )] = y_predict\n",
    "\n",
    "  # Calculate strategy returns and store them in 'Ret1' column\n",
    "  df['Ret1'] = df['Ret'] * df['Pred_Signal']\n",
    "\n",
    "  deltaTime = datetime.datetime.now() - startTime\n",
    "  print( \"Predict elapsed {:.3f}s\".format( deltaTime.total_seconds() )) # milliseconds\n",
    "\n",
    "  print( 'score=[', clf.score( X_test, y_predict ), ']' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2649c30c-0127-4a48-bcb0-7f246d00714c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build our support vector machine using a radial basis function as our kernel, the cost, or C, at 1, and the gamma function at &frac12;, or 1 over the number of inputs we are using\n",
    "#SVM <- svm( Class~RSI3+Trend, data=Training, kernel=\"radial\", cost=1, gamma=1/2 )\n",
    "\n",
    "# Run the algorithm once more over the training set to visualize the patterns it found\n",
    "#TrainingPredictions <- predict( SVM, Training, type=\"class\" )\n",
    "\n",
    "# Create a data set with the predictions\n",
    "#TrainingData <- data.frame( Training, TrainingPredictions )\n",
    "print( X_train[0:5] )\n",
    "print( y_train[0:5] )\n",
    "TrainingData = pd.concat( [X_train, y_train], axis=1, ignore_index=True )\n",
    "TrainingData.columns = [ 'Open', 'RSI3', 'SMA50', 'Trend', 'Class' ]\n",
    "print( TrainingData[0:5] )\n",
    "\n",
    "# Now lets see what patterns it was able to find\n",
    "#ggplot(\n",
    "#  TrainingData,\n",
    "#  aes( x=Trend,y=RSI3)) +\n",
    "#  stat_density2d(geom=\"contour\",aes(color=TrainingPredictions)) + \n",
    "#  labs(title=\"SVM RSI3 and Trend Predictions\", x=\"Open - SMA50\", y=\"RSI3\", color=\"Training Predictions\"\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83b7df7-13dc-401c-80a7-f21fd4088876",
   "metadata": {},
   "outputs": [],
   "source": [
    "print( len( y ), len( y_predict ))\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "#cm = confusion_matrix( y[split:], y_predict )\n",
    "#sum = 0\n",
    "#for i in range( cm.shape[0] ):\n",
    "#  sum += cm[i][i]\n",
    "#sum += cm[1][0] + cm[1][2]\n",
    "    \n",
    "#accuracy = sum / X_test.shape[0]\n",
    "#print( accuracy * 100.0 )\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions( y[split:], y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de6ac96-07c5-453b-8461-b29cc739ad7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the classification report\n",
    "cr = classification_report( y[split:], y_predict )\n",
    "print(cr)\n",
    "\n",
    "print( accuracy_score( y[split:], y_predict ))\n",
    "print( precision_score( y[split:], y_predict ))\n",
    "print( recall_score( y[split:], y_predict ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d92dd02-4c95-402c-861e-88fb93f3b69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features_v1(i):\n",
    "    df = create_HLCV(i)\n",
    "    high = df[f'high_{i}D']\n",
    "    low = df[f'low_{i}D']\n",
    "    close = df[f'close_{i}D']\n",
    "    volume = df[f'volume_{i}D']\n",
    "    \n",
    "    features = pd.DataFrame(index=prices.index)\n",
    "    features[f'volume_{i}D'] = volume\n",
    "    features[f'price_spread_{i}D'] = high - low\n",
    "    features[f'close_loc_{i}D'] = (high - close) / (high - low)\n",
    "    features[f'close_change_{i}D'] = close.pct_change()\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fa49b3-d32c-405f-8b4c-e55d7fa908ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bunch_of_features_v1():\n",
    "  '''\n",
    "  the timespan that i would like to explore \n",
    "  are 1, 2, 3 days and 1 week, 1 month, 2 month, 3 month\n",
    "  which roughly are [1,2,3,5,20,40,60]\n",
    "  '''\n",
    "  days = [1,2,3,5,20,40,60]\n",
    "  bunch_of_features = pd.DataFrame(index=prices.index)\n",
    "  for day in days:\n",
    "    f = create_features_v1(day)\n",
    "    bunch_of_features = bunch_of_features.join(f)\n",
    "    \n",
    "  return bunch_of_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93580a34-856b-4312-9e98-edade677677e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bunch_of_features_v1 = create_bunch_of_features_v1()\n",
    "\n",
    "#check the correlation\n",
    "corr_v1 = bunch_of_features_v1.corrwith(outcomes.close_1)\n",
    "corr_v1.sort_values(ascending=False).plot.barh( title = 'Strength of Correlation')\n",
    "\n",
    "corr_matrix_v1 = bunch_of_features_v1.corr()\n",
    "\n",
    "sns.clustermap(corr_matrix_v1)\n",
    "\n",
    "sns.clustermap(corr_matrix_v1, cmap='coolwarm', linewidth=1)\n",
    "sns.clustermap(corr_matrix_v1, cmap='coolwarm', linewidth=1, method='ward')\n",
    "\n",
    "#define the outcome target\n",
    "#here to make thing easy to understand, i will only try to predict #the next days's return\n",
    "outcomes = pd.DataFrame(index=prices.index)\n",
    "# next day's returns\n",
    "outcomes['close_1'] = prices.close.pct_change(-1)\n",
    "#decide which features are abundant from cluster map\n",
    "deselected_features_v1 = ['close_loc_3D','close_loc_60D',\n",
    "                       'volume_3D', 'volume_60D',\n",
    "                       'price_spread_3D','price_spread_60D',\n",
    "                       'close_change_3D','close_change_60D']\n",
    "selected_features_v1 = bunch_of_features_v1.drop(labels=deselected_features_v1, axis=1)\n",
    "\n",
    "sns.pairplot(selected_features_v1)\n",
    "\n",
    "#join the features and outcome together to remove the outliers\n",
    "features_outcomes = selected_features_v1.join(outcomes)\n",
    "stats = features_outcomes.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
